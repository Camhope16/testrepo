{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Wrangling Lab**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **45** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will perform data wrangling tasks to prepare raw data for analysis. Data wrangling involves cleaning, transforming, and organizing data into a structured format suitable for analysis. This lab focuses on tasks like identifying inconsistencies, encoding categorical variables, and feature transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab, you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identify and remove inconsistent data entries.\n",
    "\n",
    "- Encode categorical variables for analysis.\n",
    "\n",
    "- Handle missing values using multiple imputation strategies.\n",
    "\n",
    "- Apply feature scaling and transformation techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intsall the required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import the necessary module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>1.1 Import necessary libraries and load the dataset.</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the dataset is loaded correctly by displaying the first few rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age  \\\n",
      "0           1  I am a developer by profession  Under 18 years old   \n",
      "1           2  I am a developer by profession     35-44 years old   \n",
      "2           3  I am a developer by profession     45-54 years old   \n",
      "3           4           I am learning to code     18-24 years old   \n",
      "4           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "            Employment RemoteWork   Check  \\\n",
      "0  Employed, full-time     Remote  Apples   \n",
      "1  Employed, full-time     Remote  Apples   \n",
      "2  Employed, full-time     Remote  Apples   \n",
      "3   Student, full-time        NaN  Apples   \n",
      "4   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                    CodingActivities  \\\n",
      "0                                              Hobby   \n",
      "1  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Hobby;Contribute to open-source projects;Other...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3  Some college/university study without earning ...   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "0                                                NaN  ...            NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "0            NaN            NaN            NaN             NaN   \n",
      "1            0.0            0.0            0.0             0.0   \n",
      "2            NaN            NaN            NaN             NaN   \n",
      "3            NaN            NaN            NaN             NaN   \n",
      "4            NaN            NaN            NaN             NaN   \n",
      "\n",
      "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
      "0             NaN                    NaN        NaN                 NaN    NaN  \n",
      "1             0.0                    NaN        NaN                 NaN    NaN  \n",
      "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
      "3             NaN               Too long       Easy                 NaN    NaN  \n",
      "4             NaN              Too short       Easy                 NaN    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Stack Overflow survey data\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explore the Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>2.1 Summarize the dataset by displaying the column data types, counts, and missing values.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Types of each column:\n",
      "ResponseId               int64\n",
      "MainBranch              object\n",
      "Age                     object\n",
      "Employment              object\n",
      "RemoteWork              object\n",
      "                        ...   \n",
      "JobSatPoints_11        float64\n",
      "SurveyLength            object\n",
      "SurveyEase              object\n",
      "ConvertedCompYearly    float64\n",
      "JobSat                 float64\n",
      "Length: 114, dtype: object\n",
      "\n",
      "Count of non-null values in each column:\n",
      "ResponseId             65437\n",
      "MainBranch             65437\n",
      "Age                    65437\n",
      "Employment             65437\n",
      "RemoteWork             54806\n",
      "                       ...  \n",
      "JobSatPoints_11        29445\n",
      "SurveyLength           56182\n",
      "SurveyEase             56238\n",
      "ConvertedCompYearly    23435\n",
      "JobSat                 29126\n",
      "Length: 114, dtype: int64\n",
      "\n",
      "Count of missing values in each column:\n",
      "ResponseId                 0\n",
      "MainBranch                 0\n",
      "Age                        0\n",
      "Employment                 0\n",
      "RemoteWork             10631\n",
      "                       ...  \n",
      "JobSatPoints_11        35992\n",
      "SurveyLength            9255\n",
      "SurveyEase              9199\n",
      "ConvertedCompYearly    42002\n",
      "JobSat                 36311\n",
      "Length: 114, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# Display data types of each column\n",
    "print(\"Data Types of each column:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Display count of non-null values for each column\n",
    "print(\"\\nCount of non-null values in each column:\")\n",
    "print(df.count())\n",
    "\n",
    "# Display count of missing values for each column\n",
    "print(\"\\nCount of missing values in each column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>2.2 Generate basic statistics for numerical columns.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Statistics for Numerical Columns:\n",
      "         ResponseId      CompTotal       WorkExp  JobSatPoints_1  \\\n",
      "count  65437.000000   3.374000e+04  29658.000000    29324.000000   \n",
      "mean   32719.000000  2.963841e+145     11.466957       18.581094   \n",
      "std    18890.179119  5.444117e+147      9.168709       25.966221   \n",
      "min        1.000000   0.000000e+00      0.000000        0.000000   \n",
      "25%    16360.000000   6.000000e+04      4.000000        0.000000   \n",
      "50%    32719.000000   1.100000e+05      9.000000       10.000000   \n",
      "75%    49078.000000   2.500000e+05     16.000000       22.000000   \n",
      "max    65437.000000  1.000000e+150     50.000000      100.000000   \n",
      "\n",
      "       JobSatPoints_4  JobSatPoints_5  JobSatPoints_6  JobSatPoints_7  \\\n",
      "count    29393.000000    29411.000000    29450.000000     29448.00000   \n",
      "mean         7.522140       10.060857       24.343232        22.96522   \n",
      "std         18.422661       21.833836       27.089360        27.01774   \n",
      "min          0.000000        0.000000        0.000000         0.00000   \n",
      "25%          0.000000        0.000000        0.000000         0.00000   \n",
      "50%          0.000000        0.000000       20.000000        15.00000   \n",
      "75%          5.000000       10.000000       30.000000        30.00000   \n",
      "max        100.000000      100.000000      100.000000       100.00000   \n",
      "\n",
      "       JobSatPoints_8  JobSatPoints_9  JobSatPoints_10  JobSatPoints_11  \\\n",
      "count    29456.000000    29456.000000     29450.000000     29445.000000   \n",
      "mean        20.278165       16.169432        10.955713         9.953948   \n",
      "std         26.108110       24.845032        22.906263        21.775652   \n",
      "min          0.000000        0.000000         0.000000         0.000000   \n",
      "25%          0.000000        0.000000         0.000000         0.000000   \n",
      "50%         10.000000        5.000000         0.000000         0.000000   \n",
      "75%         25.000000       20.000000        10.000000        10.000000   \n",
      "max        100.000000      100.000000       100.000000       100.000000   \n",
      "\n",
      "       ConvertedCompYearly        JobSat  \n",
      "count         2.343500e+04  29126.000000  \n",
      "mean          8.615529e+04      6.935041  \n",
      "std           1.867570e+05      2.088259  \n",
      "min           1.000000e+00      0.000000  \n",
      "25%           3.271200e+04      6.000000  \n",
      "50%           6.500000e+04      7.000000  \n",
      "75%           1.079715e+05      8.000000  \n",
      "max           1.625660e+07     10.000000  \n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "# Generate basic statistics for numerical columns\n",
    "numerical_stats = df.describe()\n",
    "\n",
    "# Display the statistics\n",
    "print(\"Basic Statistics for Numerical Columns:\")\n",
    "print(numerical_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Identifying and Removing Inconsistencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>3.1 Identify inconsistent or irrelevant entries in specific columns (e.g., Country).</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ResponseId                      MainBranch                 Age  \\\n",
      "0           1  I am a developer by profession  Under 18 years old   \n",
      "1           2  I am a developer by profession     35-44 years old   \n",
      "2           3  I am a developer by profession     45-54 years old   \n",
      "3           4           I am learning to code     18-24 years old   \n",
      "4           5  I am a developer by profession     18-24 years old   \n",
      "\n",
      "            Employment RemoteWork   Check  \\\n",
      "0  Employed, full-time     Remote  Apples   \n",
      "1  Employed, full-time     Remote  Apples   \n",
      "2  Employed, full-time     Remote  Apples   \n",
      "3   Student, full-time        NaN  Apples   \n",
      "4   Student, full-time        NaN  Apples   \n",
      "\n",
      "                                    CodingActivities  \\\n",
      "0                                              Hobby   \n",
      "1  Hobby;Contribute to open-source projects;Other...   \n",
      "2  Hobby;Contribute to open-source projects;Other...   \n",
      "3                                                NaN   \n",
      "4                                                NaN   \n",
      "\n",
      "                                             EdLevel  \\\n",
      "0                          Primary/elementary school   \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)   \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)   \n",
      "3  Some college/university study without earning ...   \n",
      "4  Secondary school (e.g. American high school, G...   \n",
      "\n",
      "                                           LearnCode  \\\n",
      "0                             Books / Physical media   \n",
      "1  Books / Physical media;Colleague;On the job tr...   \n",
      "2  Books / Physical media;Colleague;On the job tr...   \n",
      "3  Other online resources (e.g., videos, blogs, f...   \n",
      "4  Other online resources (e.g., videos, blogs, f...   \n",
      "\n",
      "                                     LearnCodeOnline  ... JobSatPoints_6  \\\n",
      "0                                                NaN  ...            NaN   \n",
      "1  Technical documentation;Blogs;Books;Written Tu...  ...            0.0   \n",
      "2  Technical documentation;Blogs;Books;Written Tu...  ...            NaN   \n",
      "3  Stack Overflow;How-to videos;Interactive tutorial  ...            NaN   \n",
      "4  Technical documentation;Blogs;Written Tutorial...  ...            NaN   \n",
      "\n",
      "  JobSatPoints_7 JobSatPoints_8 JobSatPoints_9 JobSatPoints_10  \\\n",
      "0            NaN            NaN            NaN             NaN   \n",
      "1            0.0            0.0            0.0             0.0   \n",
      "2            NaN            NaN            NaN             NaN   \n",
      "3            NaN            NaN            NaN             NaN   \n",
      "4            NaN            NaN            NaN             NaN   \n",
      "\n",
      "  JobSatPoints_11           SurveyLength SurveyEase ConvertedCompYearly JobSat  \n",
      "0             NaN                    NaN        NaN                 NaN    NaN  \n",
      "1             0.0                    NaN        NaN                 NaN    NaN  \n",
      "2             NaN  Appropriate in length       Easy                 NaN    NaN  \n",
      "3             NaN               Too long       Easy                 NaN    NaN  \n",
      "4             NaN              Too short       Easy                 NaN    NaN  \n",
      "\n",
      "[5 rows x 114 columns]\n",
      "\n",
      "Unique values in 'Country' column:\n",
      "['United States of America'\n",
      " 'United Kingdom of Great Britain and Northern Ireland' 'Canada' 'Norway'\n",
      " 'Uzbekistan' 'Serbia' 'Poland' 'Philippines' 'Bulgaria' 'Switzerland'\n",
      " 'India' 'Germany' 'Ireland' 'Italy' 'Ukraine' 'Australia' 'Brazil'\n",
      " 'Japan' 'Austria' 'Iran, Islamic Republic of...' 'France' 'Saudi Arabia'\n",
      " 'Romania' 'Turkey' 'Nepal' 'Algeria' 'Sweden' 'Netherlands' 'Croatia'\n",
      " 'Pakistan' 'Czech Republic' 'Republic of North Macedonia' 'Finland'\n",
      " 'Slovakia' 'Russian Federation' 'Greece' 'Israel' 'Belgium' 'Mexico'\n",
      " 'United Republic of Tanzania' 'Hungary' 'Argentina' 'Portugal'\n",
      " 'Sri Lanka' 'Latvia' 'China' 'Singapore' 'Lebanon' 'Spain' 'South Africa'\n",
      " 'Lithuania' 'Viet Nam' 'Dominican Republic' 'Indonesia' 'Kosovo'\n",
      " 'Morocco' 'Taiwan' 'Georgia' 'San Marino' 'Tunisia' 'Bangladesh'\n",
      " 'Nigeria' 'Liechtenstein' 'Denmark' 'Ecuador' 'Malaysia' 'Albania'\n",
      " 'Azerbaijan' 'Chile' 'Ghana' 'Peru' 'Bolivia' 'Egypt' 'Luxembourg'\n",
      " 'Montenegro' 'Cyprus' 'Paraguay' 'Kazakhstan' 'Slovenia' 'Jordan'\n",
      " 'Venezuela, Bolivarian Republic of...' 'Costa Rica' 'Jamaica' 'Thailand'\n",
      " 'Nicaragua' 'Myanmar' 'Republic of Korea' 'Rwanda'\n",
      " 'Bosnia and Herzegovina' 'Benin' 'El Salvador' 'Zimbabwe' 'Afghanistan'\n",
      " 'Estonia' 'Malta' 'Uruguay' 'Belarus' 'Colombia' 'Republic of Moldova'\n",
      " 'Isle of Man' 'Nomadic' 'New Zealand' 'Palestine' 'Armenia'\n",
      " 'United Arab Emirates' 'Maldives' 'Ethiopia' 'Fiji' 'Guatemala' 'Uganda'\n",
      " 'Turkmenistan' 'Mauritius' 'Kenya' 'Cuba' 'Gabon' 'Bahamas' 'South Korea'\n",
      " 'Iceland' 'Honduras' 'Hong Kong (S.A.R.)'\n",
      " \"Lao People's Democratic Republic\" 'Mongolia' 'Cambodia' 'Madagascar'\n",
      " 'Angola' 'Democratic Republic of the Congo' 'Syrian Arab Republic' 'Iraq'\n",
      " 'Namibia' 'Senegal' 'Kyrgyzstan' 'Zambia' 'Swaziland' \"Côte d'Ivoire\"\n",
      " 'Kuwait' 'Tajikistan' 'Burundi' 'Trinidad and Tobago' 'Mauritania'\n",
      " 'Sierra Leone' 'Panama' 'Somalia' 'North Korea' 'Dominica' 'Guyana'\n",
      " 'Togo' 'Oman' 'Barbados' 'Andorra'\n",
      " \"Democratic People's Republic of Korea\" 'Qatar' 'Sudan' 'Cameroon'\n",
      " 'Papua New Guinea' 'Bahrain' 'Yemen' 'Malawi' 'Burkina Faso'\n",
      " 'Congo, Republic of the...' 'Botswana' 'Guinea-Bissau' 'Mozambique'\n",
      " 'Central African Republic' 'Equatorial Guinea' 'Suriname' 'Belize'\n",
      " 'Libyan Arab Jamahiriya' 'Cape Verde' 'Brunei Darussalam' 'Bhutan'\n",
      " 'Guinea' 'Niger' 'Antigua and Barbuda' 'Mali' 'Samoa' 'Lesotho'\n",
      " 'Saint Kitts and Nevis' 'Monaco' 'Micronesia, Federated States of...'\n",
      " 'Haiti' nan 'Nauru' 'Liberia' 'Chad' 'Djibouti' 'Solomon Islands']\n",
      "\n",
      "Missing or blank entries in 'Country' column:\n",
      "Missing values: 6507\n",
      "Blank values: 0\n",
      "\n",
      "Entries with 'Unknown', 'N/A', or 'Other' in 'Country' column:\n",
      "Empty DataFrame\n",
      "Columns: [Country]\n",
      "Index: []\n",
      "\n",
      "Summary of issues with the 'Country' column:\n",
      "Total missing or blank entries: 6507\n",
      "Total invalid country entries (e.g., 'Unknown', 'N/A', 'Other'): 0\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import pandas as pd\n",
    "\n",
    "# Display the first few rows of the dataset to inspect the structure\n",
    "print(df.head())\n",
    "\n",
    "# Check for unique values in the 'Country' column to identify potential inconsistencies or irrelevant entries\n",
    "print(\"\\nUnique values in 'Country' column:\")\n",
    "print(df['Country'].unique())\n",
    "\n",
    "# Check for missing values or blank entries in the 'Country' column\n",
    "print(\"\\nMissing or blank entries in 'Country' column:\")\n",
    "missing_values = df['Country'].isnull().sum()\n",
    "blank_values = (df['Country'] == '').sum()\n",
    "print(f\"Missing values: {missing_values}\")\n",
    "print(f\"Blank values: {blank_values}\")\n",
    "\n",
    "# Check for any entries that don't match a list of known countries (this is optional and depends on the scope)\n",
    "# Here we'll just check for entries that might not look like proper country names (e.g., \"Unknown\", \"N/A\", etc.)\n",
    "invalid_entries = df[df['Country'].str.contains('Unknown|N/A|none|Other', case=False, na=False)]\n",
    "print(\"\\nEntries with 'Unknown', 'N/A', or 'Other' in 'Country' column:\")\n",
    "print(invalid_entries[['Country']].head())\n",
    "\n",
    "# Summary of potential issues (missing, blank, and invalid entries)\n",
    "print(\"\\nSummary of issues with the 'Country' column:\")\n",
    "print(f\"Total missing or blank entries: {missing_values + blank_values}\")\n",
    "print(f\"Total invalid country entries (e.g., 'Unknown', 'N/A', 'Other'): {len(invalid_entries)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>3.2 Standardize entries in columns like Country or EdLevel by mapping inconsistent values to a consistent format.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Country' column:\n",
      "['United States of America'\n",
      " 'United Kingdom of Great Britain and Northern Ireland' 'Canada' 'Norway'\n",
      " 'Uzbekistan' 'Serbia' 'Poland' 'Philippines' 'Bulgaria' 'Switzerland'\n",
      " 'India' 'Germany' 'Ireland' 'Italy' 'Ukraine' 'Australia' 'Brazil'\n",
      " 'Japan' 'Austria' 'Iran, Islamic Republic of...' 'France' 'Saudi Arabia'\n",
      " 'Romania' 'Turkey' 'Nepal' 'Algeria' 'Sweden' 'Netherlands' 'Croatia'\n",
      " 'Pakistan' 'Czech Republic' 'Republic of North Macedonia' 'Finland'\n",
      " 'Slovakia' 'Russian Federation' 'Greece' 'Israel' 'Belgium' 'Mexico'\n",
      " 'United Republic of Tanzania' 'Hungary' 'Argentina' 'Portugal'\n",
      " 'Sri Lanka' 'Latvia' 'China' 'Singapore' 'Lebanon' 'Spain' 'South Africa'\n",
      " 'Lithuania' 'Viet Nam' 'Dominican Republic' 'Indonesia' 'Kosovo'\n",
      " 'Morocco' 'Taiwan' 'Georgia' 'San Marino' 'Tunisia' 'Bangladesh'\n",
      " 'Nigeria' 'Liechtenstein' 'Denmark' 'Ecuador' 'Malaysia' 'Albania'\n",
      " 'Azerbaijan' 'Chile' 'Ghana' 'Peru' 'Bolivia' 'Egypt' 'Luxembourg'\n",
      " 'Montenegro' 'Cyprus' 'Paraguay' 'Kazakhstan' 'Slovenia' 'Jordan'\n",
      " 'Venezuela, Bolivarian Republic of...' 'Costa Rica' 'Jamaica' 'Thailand'\n",
      " 'Nicaragua' 'Myanmar' 'Republic of Korea' 'Rwanda'\n",
      " 'Bosnia and Herzegovina' 'Benin' 'El Salvador' 'Zimbabwe' 'Afghanistan'\n",
      " 'Estonia' 'Malta' 'Uruguay' 'Belarus' 'Colombia' 'Republic of Moldova'\n",
      " 'Isle of Man' 'Nomadic' 'New Zealand' 'Palestine' 'Armenia'\n",
      " 'United Arab Emirates' 'Maldives' 'Ethiopia' 'Fiji' 'Guatemala' 'Uganda'\n",
      " 'Turkmenistan' 'Mauritius' 'Kenya' 'Cuba' 'Gabon' 'Bahamas' 'South Korea'\n",
      " 'Iceland' 'Honduras' 'Hong Kong (S.A.R.)'\n",
      " \"Lao People's Democratic Republic\" 'Mongolia' 'Cambodia' 'Madagascar'\n",
      " 'Angola' 'Democratic Republic of the Congo' 'Syrian Arab Republic' 'Iraq'\n",
      " 'Namibia' 'Senegal' 'Kyrgyzstan' 'Zambia' 'Swaziland' \"Côte d'Ivoire\"\n",
      " 'Kuwait' 'Tajikistan' 'Burundi' 'Trinidad and Tobago' 'Mauritania'\n",
      " 'Sierra Leone' 'Panama' 'Somalia' 'North Korea' 'Dominica' 'Guyana'\n",
      " 'Togo' 'Oman' 'Barbados' 'Andorra'\n",
      " \"Democratic People's Republic of Korea\" 'Qatar' 'Sudan' 'Cameroon'\n",
      " 'Papua New Guinea' 'Bahrain' 'Yemen' 'Malawi' 'Burkina Faso'\n",
      " 'Congo, Republic of the...' 'Botswana' 'Guinea-Bissau' 'Mozambique'\n",
      " 'Central African Republic' 'Equatorial Guinea' 'Suriname' 'Belize'\n",
      " 'Libyan Arab Jamahiriya' 'Cape Verde' 'Brunei Darussalam' 'Bhutan'\n",
      " 'Guinea' 'Niger' 'Antigua and Barbuda' 'Mali' 'Samoa' 'Lesotho'\n",
      " 'Saint Kitts and Nevis' 'Monaco' 'Micronesia, Federated States of...'\n",
      " 'Haiti' nan 'Nauru' 'Liberia' 'Chad' 'Djibouti' 'Solomon Islands']\n",
      "\n",
      "Unique values in 'EdLevel' column:\n",
      "['Primary/elementary school'\n",
      " 'Bachelor’s degree (B.A., B.S., B.Eng., etc.)'\n",
      " 'Master’s degree (M.A., M.S., M.Eng., MBA, etc.)'\n",
      " 'Some college/university study without earning a degree'\n",
      " 'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)'\n",
      " 'Professional degree (JD, MD, Ph.D, Ed.D, etc.)'\n",
      " 'Associate degree (A.A., A.S., etc.)' 'Something else' nan]\n",
      "\n",
      "Standardized 'Country' values:\n",
      "['United States of America'\n",
      " 'United Kingdom of Great Britain and Northern Ireland' 'Canada' 'Norway'\n",
      " 'Uzbekistan' 'Serbia' 'Poland' 'Philippines' 'Bulgaria' 'Switzerland'\n",
      " 'India' 'Germany' 'Ireland' 'Italy' 'Ukraine' 'Australia' 'Brazil'\n",
      " 'Japan' 'Austria' 'Iran, Islamic Republic of...' 'France' 'Saudi Arabia'\n",
      " 'Romania' 'Turkey' 'Nepal' 'Algeria' 'Sweden' 'Netherlands' 'Croatia'\n",
      " 'Pakistan' 'Czech Republic' 'Republic of North Macedonia' 'Finland'\n",
      " 'Slovakia' 'Russian Federation' 'Greece' 'Israel' 'Belgium' 'Mexico'\n",
      " 'United Republic of Tanzania' 'Hungary' 'Argentina' 'Portugal'\n",
      " 'Sri Lanka' 'Latvia' 'China' 'Singapore' 'Lebanon' 'Spain' 'South Africa'\n",
      " 'Lithuania' 'Viet Nam' 'Dominican Republic' 'Indonesia' 'Kosovo'\n",
      " 'Morocco' 'Taiwan' 'Georgia' 'San Marino' 'Tunisia' 'Bangladesh'\n",
      " 'Nigeria' 'Liechtenstein' 'Denmark' 'Ecuador' 'Malaysia' 'Albania'\n",
      " 'Azerbaijan' 'Chile' 'Ghana' 'Peru' 'Bolivia' 'Egypt' 'Luxembourg'\n",
      " 'Montenegro' 'Cyprus' 'Paraguay' 'Kazakhstan' 'Slovenia' 'Jordan'\n",
      " 'Venezuela, Bolivarian Republic of...' 'Costa Rica' 'Jamaica' 'Thailand'\n",
      " 'Nicaragua' 'Myanmar' 'Republic of Korea' 'Rwanda'\n",
      " 'Bosnia and Herzegovina' 'Benin' 'El Salvador' 'Zimbabwe' 'Afghanistan'\n",
      " 'Estonia' 'Malta' 'Uruguay' 'Belarus' 'Colombia' 'Republic of Moldova'\n",
      " 'Isle of Man' 'Nomadic' 'New Zealand' 'Palestine' 'Armenia'\n",
      " 'United Arab Emirates' 'Maldives' 'Ethiopia' 'Fiji' 'Guatemala' 'Uganda'\n",
      " 'Turkmenistan' 'Mauritius' 'Kenya' 'Cuba' 'Gabon' 'Bahamas' 'South Korea'\n",
      " 'Iceland' 'Honduras' 'Hong Kong (S.A.R.)'\n",
      " \"Lao People's Democratic Republic\" 'Mongolia' 'Cambodia' 'Madagascar'\n",
      " 'Angola' 'Democratic Republic of the Congo' 'Syrian Arab Republic' 'Iraq'\n",
      " 'Namibia' 'Senegal' 'Kyrgyzstan' 'Zambia' 'Swaziland' \"Côte d'Ivoire\"\n",
      " 'Kuwait' 'Tajikistan' 'Burundi' 'Trinidad and Tobago' 'Mauritania'\n",
      " 'Sierra Leone' 'Panama' 'Somalia' 'North Korea' 'Dominica' 'Guyana'\n",
      " 'Togo' 'Oman' 'Barbados' 'Andorra'\n",
      " \"Democratic People's Republic of Korea\" 'Qatar' 'Sudan' 'Cameroon'\n",
      " 'Papua New Guinea' 'Bahrain' 'Yemen' 'Malawi' 'Burkina Faso'\n",
      " 'Congo, Republic of the...' 'Botswana' 'Guinea-Bissau' 'Mozambique'\n",
      " 'Central African Republic' 'Equatorial Guinea' 'Suriname' 'Belize'\n",
      " 'Libyan Arab Jamahiriya' 'Cape Verde' 'Brunei Darussalam' 'Bhutan'\n",
      " 'Guinea' 'Niger' 'Antigua and Barbuda' 'Mali' 'Samoa' 'Lesotho'\n",
      " 'Saint Kitts and Nevis' 'Monaco' 'Micronesia, Federated States of...'\n",
      " 'Haiti' nan 'Nauru' 'Liberia' 'Chad' 'Djibouti' 'Solomon Islands']\n",
      "\n",
      "Standardized 'EdLevel' values:\n",
      "['Primary/elementary school'\n",
      " 'Bachelor’s degree (B.A., B.S., B.Eng., etc.)'\n",
      " 'Master’s degree (M.A., M.S., M.Eng., MBA, etc.)'\n",
      " 'Some college/university study without earning a degree'\n",
      " 'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)'\n",
      " 'Professional degree (JD, MD, Ph.D, Ed.D, etc.)'\n",
      " 'Associate degree (A.A., A.S., etc.)' 'Something else' nan]\n",
      "\n",
      "First few rows of the dataset after standardization:\n",
      "                                             Country  \\\n",
      "0                           United States of America   \n",
      "1  United Kingdom of Great Britain and Northern I...   \n",
      "2  United Kingdom of Great Britain and Northern I...   \n",
      "3                                             Canada   \n",
      "4                                             Norway   \n",
      "\n",
      "                                             EdLevel  \n",
      "0                          Primary/elementary school  \n",
      "1       Bachelor’s degree (B.A., B.S., B.Eng., etc.)  \n",
      "2    Master’s degree (M.A., M.S., M.Eng., MBA, etc.)  \n",
      "3  Some college/university study without earning ...  \n",
      "4  Secondary school (e.g. American high school, G...  \n"
     ]
    }
   ],
   "source": [
    "# Check unique values in 'Country' and 'EdLevel' columns\n",
    "print(\"Unique values in 'Country' column:\")\n",
    "print(df['Country'].unique())\n",
    "\n",
    "print(\"\\nUnique values in 'EdLevel' column:\")\n",
    "print(df['EdLevel'].unique())\n",
    "\n",
    "# Standardizing 'Country' column:\n",
    "country_mapping = {\n",
    "    'USA': 'United States',\n",
    "    'United Kingdom': 'United Kingdom',\n",
    "    'UK': 'United Kingdom',\n",
    "    'U.S.A.': 'United States',\n",
    "    'India': 'India',\n",
    "    'Brazil': 'Brazil',\n",
    "    'Canada': 'Canada',\n",
    "    'Australia': 'Australia',\n",
    "    # Add more country mappings as needed\n",
    "}\n",
    "\n",
    "# Apply the country mapping\n",
    "df['Country'] = df['Country'].replace(country_mapping)\n",
    "\n",
    "# Standardizing 'EdLevel' column:\n",
    "edlevel_mapping = {\n",
    "    'High School': 'High School',\n",
    "    'Some College': 'Some College',\n",
    "    'Bachelors': 'Bachelor\\'s Degree',\n",
    "    'Master\\'s': 'Master\\'s Degree',\n",
    "    'PhD': 'Doctoral Degree',\n",
    "    'Associate Degree': 'Associate Degree',\n",
    "    # Add more educational level mappings as needed\n",
    "}\n",
    "\n",
    "# Apply the edlevel mapping\n",
    "df['EdLevel'] = df['EdLevel'].replace(edlevel_mapping)\n",
    "\n",
    "# Check the standardized columns\n",
    "print(\"\\nStandardized 'Country' values:\")\n",
    "print(df['Country'].unique())\n",
    "\n",
    "print(\"\\nStandardized 'EdLevel' values:\")\n",
    "print(df['EdLevel'].unique())\n",
    "\n",
    "# Optionally, check the first few rows to ensure the mappings are applied correctly\n",
    "print(\"\\nFirst few rows of the dataset after standardization:\")\n",
    "print(df[['Country', 'EdLevel']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Encoding Categorical Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>4.1 Encode the Employment column using one-hot encoding.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 'Employment' column:\n",
      "['Employed, full-time' 'Student, full-time'\n",
      " 'Student, full-time;Not employed, but looking for work'\n",
      " 'Independent contractor, freelancer, or self-employed'\n",
      " 'Not employed, and not looking for work'\n",
      " 'Employed, full-time;Student, part-time'\n",
      " 'Employed, full-time;Independent contractor, freelancer, or self-employed'\n",
      " 'Employed, full-time;Student, full-time' 'Employed, part-time'\n",
      " 'Student, full-time;Employed, part-time'\n",
      " 'Student, part-time;Employed, part-time' 'I prefer not to say'\n",
      " 'Not employed, but looking for work' 'Student, part-time'\n",
      " 'Employed, full-time;Student, full-time;Independent contractor, freelancer, or self-employed;Employed, part-time'\n",
      " 'Employed, full-time;Independent contractor, freelancer, or self-employed;Student, part-time'\n",
      " 'Independent contractor, freelancer, or self-employed;Employed, part-time'\n",
      " 'Independent contractor, freelancer, or self-employed;Student, part-time;Employed, part-time'\n",
      " 'Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed'\n",
      " 'Student, full-time;Independent contractor, freelancer, or self-employed'\n",
      " 'Employed, full-time;Employed, part-time'\n",
      " 'Not employed, but looking for work;Independent contractor, freelancer, or self-employed'\n",
      " 'Student, full-time;Not employed, and not looking for work' 'Retired'\n",
      " 'Independent contractor, freelancer, or self-employed;Student, part-time'\n",
      " 'Employed, full-time;Independent contractor, freelancer, or self-employed;Employed, part-time'\n",
      " 'Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Student, part-time'\n",
      " 'Not employed, but looking for work;Student, part-time'\n",
      " 'Not employed, but looking for work;Not employed, and not looking for work'\n",
      " 'Independent contractor, freelancer, or self-employed;Retired'\n",
      " 'Not employed, but looking for work;Student, part-time;Employed, part-time'\n",
      " 'Student, full-time;Not employed, but looking for work;Not employed, and not looking for work'\n",
      " 'Employed, full-time;Not employed, but looking for work'\n",
      " 'Student, full-time;Not employed, and not looking for work;Student, part-time'\n",
      " 'Employed, full-time;Retired'\n",
      " 'Employed, full-time;Independent contractor, freelancer, or self-employed;Student, part-time;Employed, part-time'\n",
      " 'Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work'\n",
      " 'Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Employed, part-time'\n",
      " 'Not employed, but looking for work;Employed, part-time'\n",
      " 'Employed, full-time;Student, full-time;Employed, part-time'\n",
      " 'Independent contractor, freelancer, or self-employed;Not employed, and not looking for work'\n",
      " 'Not employed, and not looking for work;Student, part-time'\n",
      " 'Student, full-time;Independent contractor, freelancer, or self-employed;Employed, part-time'\n",
      " 'Student, full-time;Student, part-time'\n",
      " 'Student, full-time;Not employed, but looking for work;Student, part-time'\n",
      " 'Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Retired'\n",
      " 'Employed, full-time;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work'\n",
      " 'Employed, full-time;Student, full-time;Independent contractor, freelancer, or self-employed'\n",
      " 'Employed, full-time;Student, full-time;Student, part-time'\n",
      " 'Not employed, but looking for work;Retired'\n",
      " 'Employed, full-time;Student, full-time;Not employed, but looking for work'\n",
      " 'Not employed, and not looking for work;Retired'\n",
      " 'Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Retired'\n",
      " 'Employed, full-time;Not employed, but looking for work;Employed, part-time'\n",
      " 'Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Student, part-time;Employed, part-time;Retired'\n",
      " 'Employed, full-time;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Employed, part-time'\n",
      " 'Student, full-time;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work'\n",
      " 'Employed, full-time;Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Student, part-time;Employed, part-time;Retired'\n",
      " 'Employed, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed'\n",
      " 'Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Student, part-time'\n",
      " 'Student, full-time;Not employed, but looking for work;Retired'\n",
      " 'Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Student, part-time'\n",
      " 'Student, part-time;Retired'\n",
      " 'Student, full-time;Not employed, but looking for work;Not employed, and not looking for work;Student, part-time'\n",
      " 'Employed, full-time;Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Student, part-time;Employed, part-time'\n",
      " 'Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Retired'\n",
      " 'Employed, full-time;Student, full-time;Student, part-time;Employed, part-time'\n",
      " 'Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Student, part-time;Employed, part-time'\n",
      " 'Student, full-time;Not employed, but looking for work;Employed, part-time'\n",
      " 'Employed, full-time;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Student, part-time'\n",
      " 'Independent contractor, freelancer, or self-employed;Student, part-time;Retired'\n",
      " 'Student, full-time;Independent contractor, freelancer, or self-employed;Student, part-time;Employed, part-time'\n",
      " 'Employed, full-time;Independent contractor, freelancer, or self-employed;Student, part-time;Retired'\n",
      " 'Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work'\n",
      " 'Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Employed, part-time'\n",
      " 'Student, full-time;Independent contractor, freelancer, or self-employed;Student, part-time'\n",
      " 'Independent contractor, freelancer, or self-employed;Employed, part-time;Retired'\n",
      " 'Employed, full-time;Not employed, and not looking for work'\n",
      " 'Employed, full-time;Independent contractor, freelancer, or self-employed;Retired'\n",
      " 'Student, full-time;Student, part-time;Employed, part-time'\n",
      " 'Employed, part-time;Retired'\n",
      " 'Employed, full-time;Independent contractor, freelancer, or self-employed;Employed, part-time;Retired'\n",
      " 'Employed, full-time;Student, part-time;Employed, part-time'\n",
      " 'Employed, full-time;Student, full-time;Independent contractor, freelancer, or self-employed;Student, part-time;Employed, part-time;Retired'\n",
      " 'Student, full-time;Student, part-time;Retired'\n",
      " 'Student, full-time;Not employed, and not looking for work;Employed, part-time'\n",
      " 'Employed, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Employed, part-time'\n",
      " 'Not employed, but looking for work;Not employed, and not looking for work;Student, part-time;Employed, part-time'\n",
      " 'Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Employed, part-time'\n",
      " 'Employed, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Employed, part-time'\n",
      " 'Employed, full-time;Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Student, part-time;Employed, part-time'\n",
      " 'Employed, full-time;Student, full-time;Independent contractor, freelancer, or self-employed;Student, part-time;Employed, part-time'\n",
      " 'Not employed, and not looking for work;Employed, part-time'\n",
      " 'Employed, full-time;Student, full-time;Not employed, but looking for work;Student, part-time'\n",
      " 'Employed, full-time;Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Employed, part-time'\n",
      " 'Employed, full-time;Not employed, but looking for work;Not employed, and not looking for work;Employed, part-time'\n",
      " 'Student, full-time;Independent contractor, freelancer, or self-employed;Employed, part-time;Retired'\n",
      " 'Not employed, but looking for work;Student, part-time;Retired'\n",
      " 'Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Student, part-time;Retired'\n",
      " 'Employed, full-time;Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed'\n",
      " 'Not employed, but looking for work;Not employed, and not looking for work;Student, part-time'\n",
      " 'Employed, full-time;Student, full-time;Independent contractor, freelancer, or self-employed;Student, part-time;Retired'\n",
      " 'Employed, full-time;Student, full-time;Not employed, but looking for work;Student, part-time;Employed, part-time'\n",
      " 'Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Student, part-time'\n",
      " 'Employed, full-time;Student, full-time;Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Student, part-time;Employed, part-time;Retired'\n",
      " 'Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Not employed, and not looking for work;Employed, part-time'\n",
      " 'Student, full-time;Retired'\n",
      " 'Employed, full-time;Not employed, but looking for work;Student, part-time'\n",
      " 'Not employed, and not looking for work;Student, part-time;Employed, part-time'\n",
      " 'Not employed, but looking for work;Independent contractor, freelancer, or self-employed;Student, part-time;Retired']\n",
      "\n",
      "Dataset after one-hot encoding of the 'Employment' column:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['Employment_Employed', 'Employment_Unemployed', 'Employment_Freelance'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Display the first few rows of the dataset to check the result\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset after one-hot encoding of the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmployment\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf_encoded\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEmployment_Employed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEmployment_Unemployed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEmployment_Freelance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[1;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[0;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['Employment_Employed', 'Employment_Unemployed', 'Employment_Freelance'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "# Display the unique values in the 'Employment' column\n",
    "print(\"Unique values in 'Employment' column:\")\n",
    "print(df['Employment'].unique())\n",
    "\n",
    "# Apply one-hot encoding to the 'Employment' column using pd.get_dummies()\n",
    "df_encoded = pd.get_dummies(df, columns=['Employment'], drop_first=False)\n",
    "\n",
    "# Display the first few rows of the dataset to check the result\n",
    "print(\"\\nDataset after one-hot encoding of the 'Employment' column:\")\n",
    "print(df_encoded[['Employment_Employed', 'Employment_Unemployed', 'Employment_Freelance']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Handling Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>5.1 Identify columns with the highest number of missing values.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with the highest number of missing values:\n",
      "AINextMuch less integrated    64289\n",
      "AINextLess integrated         63082\n",
      "AINextNo change               52939\n",
      "AINextMuch more integrated    51999\n",
      "EmbeddedAdmired               48704\n",
      "                              ...  \n",
      "MainBranch                        0\n",
      "Age                               0\n",
      "Employment                        0\n",
      "Check                             0\n",
      "ResponseId                        0\n",
      "Length: 114, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "# Sort the columns by the number of missing values in descending order\n",
    "missing_values_sorted = missing_values_count.sort_values(ascending=False)\n",
    "\n",
    "# Display the columns with the highest number of missing values\n",
    "print(\"Columns with the highest number of missing values:\")\n",
    "print(missing_values_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>5.2 Impute missing values in numerical columns (e.g., `ConvertedCompYearly`) with the mean or median.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'ConvertedCompYearly': 42002\n",
      "Missing values in 'ConvertedCompYearly' after imputation: 0\n",
      "\n",
      "First few rows after imputation:\n",
      "   ConvertedCompYearly\n",
      "0         86155.287263\n",
      "1         86155.287263\n",
      "2         86155.287263\n",
      "3         86155.287263\n",
      "4         86155.287263\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "# Check the number of missing values in the 'ConvertedCompYearly' column\n",
    "missing_values_in_column = df['ConvertedCompYearly'].isnull().sum()\n",
    "print(f\"Missing values in 'ConvertedCompYearly': {missing_values_in_column}\")\n",
    "\n",
    "# Impute missing values using the mean of the 'ConvertedCompYearly' column\n",
    "mean_value = df['ConvertedCompYearly'].mean()\n",
    "df['ConvertedCompYearly'] = df['ConvertedCompYearly'].fillna(mean_value)\n",
    "\n",
    "# Alternatively, you can impute using the median\n",
    "# median_value = df['ConvertedCompYearly'].median()\n",
    "# df['ConvertedCompYearly'] = df['ConvertedCompYearly'].fillna(median_value)\n",
    "\n",
    "# Verify that the missing values have been imputed\n",
    "missing_values_after_imputation = df['ConvertedCompYearly'].isnull().sum()\n",
    "print(f\"Missing values in 'ConvertedCompYearly' after imputation: {missing_values_after_imputation}\")\n",
    "\n",
    "# Display the first few rows to verify the imputation\n",
    "print(\"\\nFirst few rows after imputation:\")\n",
    "print(df[['ConvertedCompYearly']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>5.3 Impute missing values in categorical columns (e.g., `RemoteWork`) with the most frequent value.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in 'RemoteWork': 10631\n",
      "Most frequent value in 'RemoteWork': Hybrid (some remote, some in-person)\n",
      "Missing values in 'RemoteWork' after imputation: 0\n",
      "\n",
      "First few rows after imputation:\n",
      "                             RemoteWork\n",
      "0                                Remote\n",
      "1                                Remote\n",
      "2                                Remote\n",
      "3  Hybrid (some remote, some in-person)\n",
      "4  Hybrid (some remote, some in-person)\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "# Check the number of missing values in the 'RemoteWork' column\n",
    "missing_values_in_remotework = df['RemoteWork'].isnull().sum()\n",
    "print(f\"Missing values in 'RemoteWork': {missing_values_in_remotework}\")\n",
    "\n",
    "# Find the most frequent value (mode) in the 'RemoteWork' column\n",
    "most_frequent_value = df['RemoteWork'].mode()[0]\n",
    "print(f\"Most frequent value in 'RemoteWork': {most_frequent_value}\")\n",
    "\n",
    "# Impute missing values with the most frequent value\n",
    "df['RemoteWork'] = df['RemoteWork'].fillna(most_frequent_value)\n",
    "\n",
    "# Verify that the missing values have been imputed\n",
    "missing_values_after_imputation = df['RemoteWork'].isnull().sum()\n",
    "print(f\"Missing values in 'RemoteWork' after imputation: {missing_values_after_imputation}\")\n",
    "\n",
    "# Display the first few rows to verify the imputation\n",
    "print(\"\\nFirst few rows after imputation:\")\n",
    "print(df[['RemoteWork']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Feature Scaling and Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>6.1 Apply Min-Max Scaling to normalize the `ConvertedCompYearly` column.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   ConvertedCompYearly\n",
      "0                  NaN\n",
      "1                  NaN\n",
      "2                  NaN\n",
      "3                  NaN\n",
      "4                  NaN\n",
      "\n",
      "Range before scaling: Min = 1.0, Max = 16256603.0\n",
      "\n",
      "Range after scaling: Min = 0.0, Max = 1.0\n",
      "\n",
      "First few rows after scaling:\n",
      "   ConvertedCompYearly  ConvertedCompYearly_Normalized\n",
      "0                  NaN                             NaN\n",
      "1                  NaN                             NaN\n",
      "2                  NaN                             NaN\n",
      "3                  NaN                             NaN\n",
      "4                  NaN                             NaN\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the dataset\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df[['ConvertedCompYearly']].head())\n",
    "\n",
    "# Check the range (min, max) of the 'ConvertedCompYearly' column before scaling\n",
    "min_value = df['ConvertedCompYearly'].min()\n",
    "max_value = df['ConvertedCompYearly'].max()\n",
    "print(f\"\\nRange before scaling: Min = {min_value}, Max = {max_value}\")\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max Scaling to the 'ConvertedCompYearly' column\n",
    "df['ConvertedCompYearly_Normalized'] = scaler.fit_transform(df[['ConvertedCompYearly']])\n",
    "\n",
    "# Check the range (min, max) of the 'ConvertedCompYearly_Normalized' column after scaling\n",
    "normalized_min = df['ConvertedCompYearly_Normalized'].min()\n",
    "normalized_max = df['ConvertedCompYearly_Normalized'].max()\n",
    "print(f\"\\nRange after scaling: Min = {normalized_min}, Max = {normalized_max}\")\n",
    "\n",
    "# Display the first few rows of the dataframe to verify the result\n",
    "print(\"\\nFirst few rows after scaling:\")\n",
    "print(df[['ConvertedCompYearly', 'ConvertedCompYearly_Normalized']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>6.2 Log-transform the ConvertedCompYearly column to reduce skewness.</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   ConvertedCompYearly\n",
      "0                  NaN\n",
      "1                  NaN\n",
      "2                  NaN\n",
      "3                  NaN\n",
      "4                  NaN\n",
      "\n",
      "Minimum value in 'ConvertedCompYearly': 1.0\n",
      "\n",
      "First few rows after log-transformation:\n",
      "   ConvertedCompYearly  ConvertedCompYearly_Log\n",
      "0                  NaN                      NaN\n",
      "1                  NaN                      NaN\n",
      "2                  NaN                      NaN\n",
      "3                  NaN                      NaN\n",
      "4                  NaN                      NaN\n",
      "\n",
      "Statistics before and after log-transformation:\n",
      "       ConvertedCompYearly  ConvertedCompYearly_Log\n",
      "count         2.343500e+04             23435.000000\n",
      "mean          8.615529e+04                10.784854\n",
      "std           1.867570e+05                 1.409507\n",
      "min           1.000000e+00                 0.000000\n",
      "25%           3.271200e+04                10.395497\n",
      "50%           6.500000e+04                11.082143\n",
      "75%           1.079715e+05                11.589623\n",
      "max           1.625660e+07                16.604010\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "\n",
    "# Display the first few rows of the dataset to inspect the 'ConvertedCompYearly' column\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df[['ConvertedCompYearly']].head())\n",
    "\n",
    "# Check if there are any non-positive values (zero or negative) in 'ConvertedCompYearly'\n",
    "min_value = df['ConvertedCompYearly'].min()\n",
    "print(f\"\\nMinimum value in 'ConvertedCompYearly': {min_value}\")\n",
    "\n",
    "# Apply a small constant to avoid taking the log of zero or negative numbers\n",
    "df['ConvertedCompYearly_Log'] = np.log(df['ConvertedCompYearly'].replace(0, np.nan))  # Replace 0 with NaN to avoid log(0)\n",
    "\n",
    "# Check the first few rows of the log-transformed column\n",
    "print(\"\\nFirst few rows after log-transformation:\")\n",
    "print(df[['ConvertedCompYearly', 'ConvertedCompYearly_Log']].head())\n",
    "\n",
    "# Display statistics to compare before and after transformation\n",
    "print(\"\\nStatistics before and after log-transformation:\")\n",
    "print(df[['ConvertedCompYearly', 'ConvertedCompYearly_Log']].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>7.1 Create a new column `ExperienceLevel` based on the `YearsCodePro` column:</h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "  YearsCodePro\n",
      "0          NaN\n",
      "1           17\n",
      "2           27\n",
      "3          NaN\n",
      "4          NaN\n",
      "\n",
      "Unique values in 'YearsCodePro':\n",
      "[nan '17' '27' '7' '11' '25' '12' '10' '3' 'Less than 1 year' '18' '37'\n",
      " '15' '20' '6' '2' '16' '8' '14' '4' '45' '1' '24' '29' '5' '30' '26' '9'\n",
      " '33' '13' '35' '23' '22' '31' '19' '21' '28' '34' '32' '40' '50' '39'\n",
      " '44' '42' '41' '36' '38' 'More than 50 years' '43' '47' '48' '46' '49']\n",
      "\n",
      "Number of NaN values after conversion: 16733\n",
      "\n",
      "First few rows after creating 'ExperienceLevel':\n",
      "   YearsCodePro ExperienceLevel\n",
      "0           NaN             NaN\n",
      "1          17.0          Expert\n",
      "2          27.0          Expert\n",
      "3           NaN             NaN\n",
      "4           NaN             NaN\n"
     ]
    }
   ],
   "source": [
    "## Write your code here\n",
    "\n",
    "\n",
    "# Check the first few rows to inspect the 'YearsCodePro' column\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df[['YearsCodePro']].head())\n",
    "\n",
    "# Check for non-numeric or invalid values in 'YearsCodePro'\n",
    "print(\"\\nUnique values in 'YearsCodePro':\")\n",
    "print(df['YearsCodePro'].unique())\n",
    "\n",
    "# Convert 'YearsCodePro' to numeric, coercing errors to NaN\n",
    "df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n",
    "\n",
    "# Check for NaN values after conversion\n",
    "print(\"\\nNumber of NaN values after conversion:\", df['YearsCodePro'].isna().sum())\n",
    "\n",
    "# Define the bins and labels for the ExperienceLevel column\n",
    "bins = [0, 2, 5, 10, float('inf')]  # 0-2 years, 3-5 years, 6-10 years, 11+ years\n",
    "labels = ['Beginner', 'Intermediate', 'Advanced', 'Expert']\n",
    "\n",
    "# Create the ExperienceLevel column based on YearsCodePro\n",
    "df['ExperienceLevel'] = pd.cut(df['YearsCodePro'], bins=bins, labels=labels, right=True)\n",
    "\n",
    "# Display the first few rows of the new 'ExperienceLevel' column\n",
    "print(\"\\nFirst few rows after creating 'ExperienceLevel':\")\n",
    "print(df[['YearsCodePro', 'ExperienceLevel']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you:\n",
    "\n",
    "- Explored the dataset to identify inconsistencies and missing values.\n",
    "\n",
    "- Encoded categorical variables for analysis.\n",
    "\n",
    "- Handled missing values using imputation techniques.\n",
    "\n",
    "- Normalized and transformed numerical data to prepare it for analysis.\n",
    "\n",
    "- Engineered a new feature to enhance data interpretation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "1e8e234f19fd098e27b0518a87f18de690e1c51f1d3263d5690927d19971251e"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
